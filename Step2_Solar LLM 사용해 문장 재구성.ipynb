{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d0789d3-f548-4b4a-b07f-0e006f116546",
   "metadata": {},
   "source": [
    "# Step 2. 전문가의 말을 쉽게 풀어 재구성된 text 생성\n",
    "1. UpStage의 자연어처리api, Solar LLM 사용.\n",
    "   : Solar LLM api를 사용하여 텍스트를 분석하고, 더 쉬운 단어와 문장으로 재구성\n",
    "2. 문서 검색 기반 RAG모델 구축\n",
    "   : (1) 사전에 구축한 의료 문서 DB or (2) 헬스케어 분야를 특정하는 프롬프트를 이용한 웹크롤링을 통해 관련 정보 검색 후 --> 이를 텍스트로 통합해 재구성; embeding도 upstage embeding 사용할 예정이라 A-1 구조로 설계할 예정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547aec09-1502-4760-bb6c-82e35da08bbc",
   "metadata": {},
   "source": [
    "* 1단계. Solar LLM model을 활용해 원본 텍스트를 분석하고, 이를 더 쉬운 단어와 문장으로 유연하게 재구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcf5c436-07e9-4d80-993c-29054125e167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting reportlab\n",
      "  Downloading reportlab-4.2.2-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\parkh\\anaconda3\\envs\\wiset\\lib\\site-packages (from reportlab) (10.3.0)\n",
      "Requirement already satisfied: chardet in c:\\users\\parkh\\anaconda3\\envs\\wiset\\lib\\site-packages (from reportlab) (5.2.0)\n",
      "Downloading reportlab-4.2.2-py3-none-any.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.9 MB 163.8 kB/s eta 0:00:12\n",
      "    --------------------------------------- 0.0/1.9 MB 178.6 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.1/1.9 MB 281.8 kB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 695.5 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.4/1.9 MB 1.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.5/1.9 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.6/1.9 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/1.9 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.9/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.9/1.9 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.0/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.1/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 866.3 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 866.3 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 866.3 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 866.3 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 866.3 kB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.2/1.9 MB 752.8 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/1.9 MB 800.3 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.4/1.9 MB 802.3 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.4/1.9 MB 823.8 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.5/1.9 MB 852.8 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.6/1.9 MB 881.6 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.7/1.9 MB 903.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/1.9 MB 918.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.9/1.9 MB 951.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 966.2 kB/s eta 0:00:00\n",
      "Installing collected packages: reportlab\n",
      "Successfully installed reportlab-4.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: celery 5.0.5 has a non-standard dependency specifier pytz>dev. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "#!pip install reportlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58f33802-b0d5-4a92-9085-58e9034087f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "간소화된 의사 소견:\n",
      "I. 소개\n",
      "\n",
      "* 환자의 보험은 2차 소견을 구하는 특정한 보험을 사용합니다.\n",
      "\n",
      "II. 2차 소견 요청\n",
      "\n",
      "* 환자는 본인의 주치의에게 다른 의사가 전문의의 추천을 요청할 수 있습니다.\n",
      "* 의사들은 대부분 다른 의견을 환영합니다.\n",
      "* 두 번째 의사는 첫 번째 의사와 동일한 의견을 가지지 않을 수 있습니다.\n",
      "\n",
      "III. 두 번째 의사 선택\n",
      "\n",
      "* 환자는 첫 번째 의사와 동일한 의견을 가지지 않는 두 번째 의사를 선임할 수 있습니다.\n",
      "* 두 번째 의사는 대학병원, 전문의료협회, 또는 보험회사에서 선임할 수 있습니다.\n",
      "\n",
      "IV. 환자 행동 및 주의사항\n",
      "\n",
      "* 환자는 본인이 신뢰하는 다른 의사에게 2차 소견을 요청할 수 있습니다.\n",
      "* 두 번째 의사를 선임할 수 없는 경우, 대학병원, 전문의료협회, 또는 보험회사에서 선임할 수 있습니다.\n",
      "\n",
      "V. 치료 계획\n",
      "\n",
      "* 구체적인 치료 계획은 환자의 상태와 의사의 판단에 따라 달라집니다.\n",
      "\n",
      "중요한 의학 정보:\n",
      "\n",
      "* 환자의 보험은 2차 소견을 구하는 특정한 보험을 사용합니다.\n",
      "* 환자는 주치의에게 다른 의사가 전문의의 추천을 요청할 수 있습니다.\n",
      "* 두 번째 의사는 첫 번째 의사와 동일한 의견을 가지지 않을 수 있습니다.\n",
      "* 환자는 두 번째 의사를 선임할 수 있습니다.\n",
      "\n",
      "환자가 취해야 할 행동:\n",
      "\n",
      "* 주치의에게 2차 소견을 요청할 수 있습니다.\n",
      "* 두 번째 의사를 선임할 수 없는 경우, 대학병원, 전문의료협회, 또는 보험회사에서 선임할 수 있습니다.\n",
      "\n",
      "환자가 주의해야 할 사항:\n",
      "\n",
      "* 두 번째 의사는 첫 번째 의사와 동일한 의견을 가지지 않을 수 있습니다.\n",
      "* 두 번째 의사를 선임할 수 없는 경우, 다른 방법을 찾아야 합니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_upstage import ChatUpstage\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.pdfbase import pdfmetrics\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "def read_medical_report(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        return data.get('transcription', '')\n",
    "\n",
    "def simplify_medical_report(llm, medical_report):\n",
    "    template = \"\"\"\n",
    "    아래는 환자 진료 중 의사의 소견입니다. 이 내용을 비전문가인 일반인도 쉽게 이해할 수 있도록 간단하고 명확한 언어로 다시 설명해주세요. 다음의 지침을 따라주세요.\n",
    "\n",
    "    0. 의사의 발음은 정확하지 않습니다. 따라서 발음상 text로 잘못 옮겨진 부분이 있을텐데, 우선 문맥을 파악하여 그러한 부분을 수정해주세요.\n",
    "    1. 의학 용어는 가능한 쉬운 말로 바꾸고, 필요시 간단한 설명을 덧붙이세요. \n",
    "    2. 전문적인 내용은 일상생활의 비유나 예시를 들어 설명해주세요. \n",
    "    3. 환자의 상태와 치료 계획을 가능한 경우 단계별로 설명해주세요. \n",
    "    4. 중요한 의학 정보는 반드시 포함하고, 누락되지 않도록 주의해주세요. \n",
    "    5. 환자가 취해야 할 행동이나 주의사항을 명확히 강조해주세요. \n",
    "    6. 전체 내용을 3-5개의 주요 섹션으로 나누어 구조화해주세요. \n",
    "\n",
    "    의사 소견서:\n",
    "    {medical_report}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt_template = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    prompt = prompt_template.format_messages(medical_report=medical_report)\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# 생성된 텍스트를 TXT 파일 형태로 저장\n",
    "def save_to_txt(text, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "#생성된 텍스트를 PDF 파일 형태로 저장\n",
    "def save_to_pdf(text, file_path):\n",
    "    #폰트 파일을 직접 로드\n",
    "    pdfmetrics.registerFont(TTFont('NanumGothic', 'NanumGothic.ttf'))\n",
    "    c = canvas.Canvas(file_path, pagesize=letter)\n",
    "    c.setFont('NanumGothic', 12)\n",
    "\n",
    "    #텍스트 쓰기\n",
    "    text_object = c.beginText(50, 750)\n",
    "    text_object.setFont('NanumGothic', 12)\n",
    "\n",
    "    for line in text.split('\\n'):\n",
    "        text_object.textLine(line)\n",
    "\n",
    "    c.drawText(text_object)\n",
    "    c.save()    \n",
    "\n",
    "def main():\n",
    "    # API 키를 환경 변수에서 가져옵니다\n",
    "    api_key = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"UPSTAGE_API_KEY not found in .env file\")\n",
    "\n",
    "    # Solar LLM 모델 초기화\n",
    "    llm = ChatUpstage(api_key=api_key)\n",
    "\n",
    "    file_path = \"transcription.json\"\n",
    "\n",
    "    medical_report = read_medical_report(file_path)\n",
    "    simplified_report = simplify_medical_report(llm, medical_report)\n",
    "\n",
    "    #생성된 텍스트를 파일에 저장\n",
    "    save_to_pdf(simplified_report.content, \"simplified_medical_report.pdf\")\n",
    "    save_to_txt(simplified_report.content, \"simplified_medical_report.txt\")\n",
    "    \n",
    "    print(\"간소화된 의사 소견:\")\n",
    "    print(simplified_report.content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fc09934-08b0-4600-97f5-cf0fcbb83896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요약된 의사 소견이 'simplified_medical_report_summary.pdf'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "#Step 5에서 context(입력값)에 대한 토큰수 제한(<32000)이 있어서 글자수에 제한을 두고 요약본을 추가 생성\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_upstage import ChatUpstage\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.pdfbase import pdfmetrics\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "def read_simplified_report(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def summarize_report(llm, simplified_report):\n",
    "    template = \"\"\"\n",
    "    다음은 간소화된 의사 소견입니다. 이 내용을 32000자 이내로 요약해주세요. \n",
    "    중요한 의학 정보와 환자가 취해야 할 행동이나 주의사항을 누락하지 않도록 주의해주세요.\n",
    "\n",
    "    간소화된 의사 소견:\n",
    "    {simplified_report}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt_template = ChatPromptTemplate.from_template(template)\n",
    "    prompt = prompt_template.format_messages(simplified_report=simplified_report)\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "def save_to_pdf(text, file_path):\n",
    "    pdfmetrics.registerFont(TTFont('NanumGothic', 'NanumGothic.ttf'))\n",
    "    c = canvas.Canvas(file_path, pagesize=letter)\n",
    "    c.setFont('NanumGothic', 12)\n",
    "    text_object = c.beginText(50, 750)\n",
    "    text_object.setFont('NanumGothic', 12)\n",
    "    for line in text.split('\\n'):\n",
    "        text_object.textLine(line)\n",
    "    c.drawText(text_object)\n",
    "    c.save()\n",
    "\n",
    "def main():\n",
    "    # API 키를 환경 변수에서 가져옵니다\n",
    "    api_key = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"UPSTAGE_API_KEY not found in .env file\")\n",
    "\n",
    "    # Solar LLM 모델 초기화\n",
    "    llm = ChatUpstage(api_key=api_key)\n",
    "\n",
    "    # 간소화된 의사 소견 읽기\n",
    "    simplified_report = read_simplified_report(\"simplified_medical_report.txt\")\n",
    "\n",
    "    # 간소화된 보고서 요약\n",
    "    summarized_report = summarize_report(llm, simplified_report)\n",
    "\n",
    "    # 요약된 보고서를 PDF 파일로 저장\n",
    "    save_to_pdf(summarized_report, \"simplified_medical_report_summary.pdf\")\n",
    "\n",
    "    print(\"요약된 의사 소견이 'simplified_medical_report_summary.pdf'로 저장되었습니다.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
